# Book Modelling

Getting a book published is not a game of chance. At least, [that is the position of editors](http://www.macgregorandluedeke.com/blog/ask-agent-odds-getting-published/), the people who ultimately determine which books are going to be selected for publication. Editors believe that published books are not published at random, but instead possess some particular quality or set of qualities that makes them appealing for publication, qualities that rejected manuscripts - in theory - do not possess. In other words, published books are not selected at random, they are carefully chosen from a much bigger set of manuscripts because those particular words written in that particular manner enticed and entranced the editor as they read through them.

If that is true, that would constitute a **pattern**, and patterns are the bread and butter of machine learning algorithms. If there is a true pattern that distinguishes published books from other kinds of books, then it is theoretically possible to classify books according to it. A classificator trained in such a manner would be able to tell between books that have potential for publishing and those that do not meet such criterion.

Creating such a classificator is the goal of this project.

To the best of my knowledge, this is the first attempt to create such an algorithm, and one of the most challenging aspects in tackling this issue is the lack of a database. One would need thousands of books correctly tagged with the label of "published" or "rejected" to do a proper modelling of the problem, and no such database seems to exist. To circumvent this problem, one would need to create their own database. And that is precisely what was done for this project.

The source for all data used here was the Amazon website, which contains thousand upons thousands of books in multiple languages, formats and genres. Importantly, the Amazon website also offers free book previews from all their books with the "look inside" feature in the book's webpage, and it also informs which publisher approved the book, if any. Unfortunately, this is all unstructered data, so it had to be organized from scratch.

In order to accomplish that, a simple webscraper was developed to navigate the Amazon website and store this freely-available information in a structured manner. This webscraper would access a book's webpage and retrieve information such as author's name, book title, Amazon rank, number of reviews, average rating, number of pages and many other features. Importantly, the webscraper would also use the "look inside" feature to store the book's preview content in the database, and it would store any publisher information, if it could find any (more on this below).

Two important aspects of the this data collecting process must be stressed out, regarding the type of book that was stored and the Publisher information.

- Type of book: only a small set of all books available on Amazon was selected for this project. Specifically, only fantasy books from the subgenre of Sword and Sorcery, written or translated to the English language, available as ebooks, from the years of 2017, 2018 and 2019. These limitations were artificially imposed for two reasons:
  - So that the classificator could be trained inside only one subgenre of books in one language, getting a more uniform database from which it could hopefully infer more meaningful patterns that differentiate published books from other kinds of books.
  - Due to the fact that every piece of book content had to go through an additional preprocessing step that limited the number of books one could get from the website (more on this below).
- Publisher information: the webscraper would search for a field in the book's webpage that was labeled as "Publisher", storing any information that was contained there. However, this field was not always present. The lack of such information generally meant that the book was self-published, instead of traditionally published by some publishing house. It is possible that in some cases the information was simply missing, but for the purposes of this analysis all books missing publisher information were treated as self-published books.

Before continuing, it is important to clarify a crucial change in the direction of this project upon creation of the database. The goal of creating a classificator of published books was to distinguish between publish and rejected manuscripts, but obtaining rejected manuscripts in sufficient volume to develop a classificator is not possible for practical reasons. Additionally, and obviously, all the books available on Amazon are "published" books, so the initial idea as it was originally conceived had to be abandoned. Instead, **this project will seek to differentiate between traditionally published and self-published books**. This is one possible way to try and approach this problem given the lack of a database. This begs the question: is there a difference between a traditionally published book and a self-published one?

It is hard to answer such a question, as it relates to the ancient problem of producing and evaluating works of art. Particularly in the world of publishing, being rejected once does not mean your book could not be accepted later on. [Harry Potter was famously rejected 12 times before a publisher decided to put it out into the world](https://www.today.com/popculture/j-k-rowling-s-original-harry-potter-pitch-was-rejected-t117763). However, when one focuses beyond particular cases and looks at general trends, it seems fair to answer: "yes, there probably is a difference between traditionally published and self-published books." The simple reason for that is that in the first case the writer's work had to undergo some scrutiny and selection by a third party, while in the second case that might not have happened, at least not necessarily. Some self-published authors will pay editors to help polish their manuscripts, but the fact remains that someone that got traditionally published passed through a filter for some reason, while someone that self-published did not. If the reason for passing such a filter is consistent in any way, then a machine learning algorithm may be able to detect it.

Another point of clarification must be made here regarding **how** the labels of 'traditionally published' and 'self-published' were crafted for this project. It was already explained that when the webscraper could not find any Publisher information, the book would be classified as 'self-published'. That was the easy part. The real issue came when crafting the labels for the books that *did have* a publisher name, simply because many books that contained publisher information were not traditionally published in the manner that interested this project. For instance: some books had the author name in the place where the Publisher was supposed to be (presumably because the authors incorretly filled that space with their names when putting their books on Amazon); some books had a publisher name that was in fact the name of a self-publishing service, such as Xlibris; and finally, some books had an actual publisher's name, but the publisher was founded by the author and only published books of that author - which for all intents and purposes is the same thing as a self-publication. Indeed, in all these three previous scenarios the books would be classified as self-published, and every single one of the roughly 2000 publishers present in this project had to be inspected in such a manner.

Publishers that only had works of a small group of authors (usually the ones that founded it) were also considered self-publishing. Publishers that only published work from previous acclaimed authors (such as Robert E. Howard) were classified as traditional publishers. Publishers that were in fact a cooperative of writers were considered traditional publishers if they disclosed in their site some selection system for approving writers in their cooperative - otherwise, they would be marked as self-publishing. There were many more corner cases, and certainly not all classifications were adequately dispensed, but hopefully they were sufficiently good to help distinguish a pattern.

In short, "published" books were classified as such if they seemed to be part of a third party selection system, even if the publisher was reasonably small. Converserly, even a big, best-selling author with many fans and a large website would be classified as self-publishing if they published all their books through their own private publisher.

Finally, let's return to the books, their content, and the pre-processing that was made. Since the books were collected from the free Amazon preview, they contained text information that was of no interest to this project, such as a summary, preface, general information and other pieces of text that did not contain the actual story the book was written to tell. All this information had to be manually removed. For that, the author of this project had the help of a group of colleagues to run the webscraper and manually clear the database. With this collective effort, roughly 10,500 books were acquired and, after cleaning, roughly 9,500 books remained.

After creating such database, one extra step needed to be taken, which involved removing the books that did not reach a certain number of pages, as well as removing any amount of extra pages for the books that surpassed such a number (this was a cut off point for the database of books). This number of pages was set at 15, because it was empirically verified that more pages per book improved the performance of the classificator, but selecting too high a cut off can drastically decrease the size of the database. So, at last, the prepared database contained the first 15 pages of 7427 books, plus additional variables such as the book's title, date of publication, publisher name, etc. One important takeaway is that this project **did not use the full length of pages from the books, but rather a fixed 15-page length that it had access through the free Amazon preview.**

Hopefully, with all these clarifications, you can enjoy the results obtained by the project when reading through the code.
